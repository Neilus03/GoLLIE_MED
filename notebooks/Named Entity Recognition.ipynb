{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2afe3b",
   "metadata": {},
   "source": [
    "<img src=\"../assets/CoLLIE_blue.png\" alt=\"GoLLIE\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003b73a2",
   "metadata": {},
   "source": [
    "# Named Entity Recognition with GoLLIE\n",
    "\n",
    "This notebook is an example of how to run Named Entity Recognition with GoLLIE. This notebook covers:\n",
    "\n",
    "- How to define the guidelines for a task\n",
    "- How to load GoLLIE\n",
    "- How to generate model inputs\n",
    "- How to parse the output\n",
    "- How to implement a scorer and evaluate the output\n",
    "\n",
    "You can modify this notebook to run any Named Entity Recognition task you want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b015c64",
   "metadata": {},
   "source": [
    "### Import requeriments\n",
    "\n",
    "See the requeriments.txt file in the main directory to install the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed51491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") # Add the GoLLIE base directory to sys path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ff498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hhome/nlp2_g09/miniconda3/envs/Gollie/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import rich\n",
    "import logging\n",
    "from src.model.load_model import load_model\n",
    "import black\n",
    "import inspect\n",
    "from jinja2 import Template\n",
    "import tempfile\n",
    "from src.tasks.utils_typing import AnnotationList\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from typing import Dict, List, Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004626bb",
   "metadata": {},
   "source": [
    "## Load GoLLIE\n",
    "\n",
    "We will load GOLLIE-7B from the huggingface-hub.\n",
    "You can use the function AutoModelForCausalLM.from_pretrained if you prefer it. However, we provide a handy load_model function with many functionalities already implemented that will assist you in reproducing our results.\n",
    "\n",
    "Please note that setting use_flash_attention=True is mandatory. Our flash attention implementation has small numerical differences compared to the attention implementation in Huggingface. Using use_flash_attention=False will result in the model producing inferior results. Flash attention requires an available CUDA GPU. Running GOLLIE pre-trained models on a CPU is not supported. We plan to address this in future releases.\n",
    "\n",
    "- Set force_auto_device_map=True to automatically load the model on available GPUs.\n",
    "- Set quantization=4 if the model doesn't fit in your GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb841c40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading model model from HiTZ/GoLLIE-7B\n",
      "WARNING:root:Using auto device map, we will split the model across GPUs and CPU to fit the model in memory.\n",
      "INFO:root:We will load the model using the following device map: auto and max_memory: None\n",
      "/hhome/nlp2_g09/miniconda3/envs/Gollie/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO:root:Loading model with dtype: torch.bfloat16\n",
      "WARNING:root:Model HiTZ/GoLLIE-7B is an decoder-only model. We will load it as a CausalLM model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Flash Attention installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Using Flash Attention for LLaMA model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Flash RoPE installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/hhome/nlp2_g09/miniconda3/envs/Gollie/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.78it/s]\n",
      "INFO:root:Model dtype: torch.bfloat16\n",
      "INFO:root:Total model memory footprint: 13477.101762 MB\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(\n",
    "    inference=True,\n",
    "    model_weights_name_or_path=\"HiTZ/GoLLIE-7B\",\n",
    "    quantization=None,\n",
    "    use_lora=False,\n",
    "    force_auto_device_map=True,\n",
    "    use_flash_attention=True,\n",
    "    torch_dtype=\"bfloat16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a662bf3",
   "metadata": {},
   "source": [
    "## Define the guidelines\n",
    "\n",
    "First, we will define the labels and guidelines for the task. We will represent them as Python classes.\n",
    "\n",
    "The following guidelines have been defined for this example. They were not part of the pre-training dataset. Therefore, we will run GOLLIE in zero-shot settings using unseen labels.\n",
    "\n",
    "ðŸ’¡ Be creative and try to define your own guidelines to test GoLLIE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3381513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from src.tasks.utils_typing import Entity, dataclass\n",
    "\n",
    "\"\"\"\n",
    "Entity definitions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PrivateSpaceCompany(Entity):\n",
    "    \"\"\"Refers to private companies primarily focused on space exploration, transportation, \n",
    "    satellite launch, or space-based services. These are non-governmental entities that have \n",
    "    a commercial interest in space activities.\"\"\"\n",
    "\n",
    "    span: str  # Such as: \"Blue origin\", \"Boeing\", \"Northrop Grumman\", \"Arianespace\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PublicSpaceCompany(Entity):\n",
    "    \"\"\"Refers to governmental entities or agencies that are primarily focused on space \n",
    "    exploration, research, transportation, satellite launch, or other space-based services. \n",
    "    These entities are state-owned and operated and are generally funded through public funds.\"\"\"\n",
    "\n",
    "    span: str  # Such as \"ESA\", \"ISRO\", \"CNSA\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Planet(Entity):\n",
    "    \"\"\"Refers to celestial bodies that orbit a star. Planets are large enough \n",
    "    to have cleared their orbits of other debris and have a nearly round shape \n",
    "    due to their self-gravity.\"\"\"\n",
    "\n",
    "    span: str  # Such as: \"Earth\", \"Jupiter\", \"Venus\", \"Mercury\", \"Saturn\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Launcher(Entity):\n",
    "    \"\"\"Refers to a vehicle designed primarily to transport payloads from the Earth's \n",
    "    surface to space. Launchers can carry various payloads, including satellites, \n",
    "    crewed spacecraft, and cargo, into various orbits or even beyond Earth's orbit. \n",
    "    They are usually multi-stage vehicles that use rocket engines for propulsion.\"\"\"\n",
    "\n",
    "    span: str  # Such as: \"Sturn V\", \"Atlas V\", \"Soyuz\", \"Ariane 5\"\n",
    "\n",
    "\n",
    "\n",
    "ENTITY_DEFINITIONS: List[Entity] = [\n",
    "    PrivateSpaceCompany,\n",
    "    PublicSpaceCompany,\n",
    "    Planet,\n",
    "    Launcher,\n",
    "]\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    cell_txt = In[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f8ef55",
   "metadata": {},
   "source": [
    "### Print the guidelines to guidelines.py\n",
    "\n",
    "Due to IPython limitations, we must write the content of the previous cell to a file and then import the content from that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4736a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"guidelines.py\",\"w\",encoding=\"utf8\") as python_guidelines:\n",
    "    print(cell_txt,file=python_guidelines)\n",
    "\n",
    "from guidelines import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac3db6f",
   "metadata": {},
   "source": [
    "We use inspect.getsource to get the guidelines as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89454475",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidelines = [inspect.getsource(definition) for definition in ENTITY_DEFINITIONS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bd26b7",
   "metadata": {},
   "source": [
    "## Define input sentence\n",
    "\n",
    "Here we define the input sentence and the gold labels.\n",
    "\n",
    "You can define and empy list as gold labels if you don't have gold annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d1f1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"SpaceX is colaborating with NASA in the mission to bring humans to Mars using their new Starship rocket.\"\n",
    "gold = [\n",
    "    PrivateSpaceCompany(span=\"SpaceX\"),\n",
    "    PublicSpaceCompany(span=\"NASA\"),\n",
    "    Planet(span=\"Mars\"),\n",
    "    Launcher(span=\"Starship\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90501322",
   "metadata": {},
   "source": [
    "## Filling a template\n",
    "\n",
    "For NER we will use the following prompt template.\n",
    "We use Jinja templates, which are easy to implement and exceptionally fast. For more information, visit: https://jinja.palletsprojects.com/en/3.1.x/api/#high-level-api.\n",
    "\n",
    "```Python\n",
    "# The following lines describe the task definition\n",
    "{%- for definition in guidelines %}\n",
    "{{ definition }}\n",
    "{%- endfor %}\n",
    "\n",
    "# This is the text to analyze\n",
    "text = {{ text.__repr__() }}\n",
    "\n",
    "# The annotation instances that take place in the text above are listed here\n",
    "result = [\n",
    "{%- for ann in annotations %}\n",
    "    {{ ann }},\n",
    "{%- endfor %}\n",
    "]\n",
    "\n",
    "```\n",
    "\n",
    "This template is stored in `templates/prompt.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f54034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read template\n",
    "with open(\"../templates/prompt.txt\", \"rt\") as f:\n",
    "    template = Template(f.read())\n",
    "# Fill the template\n",
    "formated_text = template.render(guidelines=guidelines, text=text, annotations=gold, gold=gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886dbc0c",
   "metadata": {},
   "source": [
    "### Black Code Formatter\n",
    "\n",
    "We use the Black Code Formatter to automatically unify all the prompts to the same format. \n",
    "\n",
    "https://github.com/psf/black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8994924",
   "metadata": {},
   "outputs": [],
   "source": [
    "black_mode = black.Mode()\n",
    "formated_text = black.format_str(formated_text, mode=black_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aa5c0e",
   "metadata": {},
   "source": [
    "### Print the filled and formatted template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa5f3106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"># The following lines describe the task definition\n",
       "@dataclass\n",
       "class <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PrivateSpaceCompany</span><span style=\"font-weight: bold\">(</span>Entity<span style=\"font-weight: bold\">)</span>:\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"Refers to private companies primarily focused on space exploration, transportation,\n",
       "    satellite launch, or space-based services. These are non-governmental entities that have\n",
       "    a commercial interest in space activities.<span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"\n",
       "\n",
       "    span: str  # Such as: <span style=\"color: #008000; text-decoration-color: #008000\">\"Blue origin\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Boeing\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Northrop Grumman\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Arianespace\"</span>\n",
       "\n",
       "\n",
       "@dataclass\n",
       "class <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PublicSpaceCompany</span><span style=\"font-weight: bold\">(</span>Entity<span style=\"font-weight: bold\">)</span>:\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"Refers to governmental entities or agencies that are primarily focused on space\n",
       "    exploration, research, transportation, satellite launch, or other space-based services.\n",
       "    These entities are state-owned and operated and are generally funded through public funds.\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"\n",
       "\n",
       "    span: str  # Such as <span style=\"color: #008000; text-decoration-color: #008000\">\"ESA\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"ISRO\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"CNSA\"</span>\n",
       "\n",
       "\n",
       "@dataclass\n",
       "class <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Planet</span><span style=\"font-weight: bold\">(</span>Entity<span style=\"font-weight: bold\">)</span>:\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"Refers to celestial bodies that orbit a star. Planets are large enough\n",
       "    to have cleared their orbits of other debris and have a nearly round shape\n",
       "    due to their self-gravity.<span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"\n",
       "\n",
       "    span: str  # Such as: <span style=\"color: #008000; text-decoration-color: #008000\">\"Earth\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Jupiter\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Venus\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Mercury\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Saturn\"</span>\n",
       "\n",
       "\n",
       "@dataclass\n",
       "class <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Launcher</span><span style=\"font-weight: bold\">(</span>Entity<span style=\"font-weight: bold\">)</span>:\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"Refers to a vehicle designed primarily to transport payloads from the Earth's\n",
       "    surface to space. Launchers can carry various payloads, including satellites,\n",
       "    crewed spacecraft, and cargo, into various orbits or even beyond Earth's orbit.\n",
       "    They are usually multi-stage vehicles that use rocket engines for propulsion.<span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"\n",
       "\n",
       "    span: str  # Such as: <span style=\"color: #008000; text-decoration-color: #008000\">\"Sturn V\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Atlas V\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Soyuz\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Ariane 5\"</span>\n",
       "\n",
       "\n",
       "# This is the text to analyze\n",
       "text = <span style=\"color: #008000; text-decoration-color: #008000\">\"SpaceX is colaborating with NASA in the mission to bring humans to Mars using their new Starship rocket.\"</span>\n",
       "\n",
       "# The annotation instances that take place in the text above are listed here\n",
       "result = <span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PrivateSpaceCompany</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">span</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"SpaceX\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PublicSpaceCompany</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">span</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"NASA\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Planet</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">span</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Mars\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Launcher</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">span</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Starship\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "# The following lines describe the task definition\n",
       "@dataclass\n",
       "class \u001b[1;35mPrivateSpaceCompany\u001b[0m\u001b[1m(\u001b[0mEntity\u001b[1m)\u001b[0m:\n",
       "    \u001b[32m\"\"\u001b[0m\"Refers to private companies primarily focused on space exploration, transportation,\n",
       "    satellite launch, or space-based services. These are non-governmental entities that have\n",
       "    a commercial interest in space activities.\u001b[32m\"\"\u001b[0m\"\n",
       "\n",
       "    span: str  # Such as: \u001b[32m\"Blue origin\"\u001b[0m, \u001b[32m\"Boeing\"\u001b[0m, \u001b[32m\"Northrop Grumman\"\u001b[0m, \u001b[32m\"Arianespace\"\u001b[0m\n",
       "\n",
       "\n",
       "@dataclass\n",
       "class \u001b[1;35mPublicSpaceCompany\u001b[0m\u001b[1m(\u001b[0mEntity\u001b[1m)\u001b[0m:\n",
       "    \u001b[32m\"\"\u001b[0m\"Refers to governmental entities or agencies that are primarily focused on space\n",
       "    exploration, research, transportation, satellite launch, or other space-based services.\n",
       "    These entities are state-owned and operated and are generally funded through public funds.\n",
       "    \u001b[32m\"\"\u001b[0m\"\n",
       "\n",
       "    span: str  # Such as \u001b[32m\"ESA\"\u001b[0m, \u001b[32m\"ISRO\"\u001b[0m, \u001b[32m\"CNSA\"\u001b[0m\n",
       "\n",
       "\n",
       "@dataclass\n",
       "class \u001b[1;35mPlanet\u001b[0m\u001b[1m(\u001b[0mEntity\u001b[1m)\u001b[0m:\n",
       "    \u001b[32m\"\"\u001b[0m\"Refers to celestial bodies that orbit a star. Planets are large enough\n",
       "    to have cleared their orbits of other debris and have a nearly round shape\n",
       "    due to their self-gravity.\u001b[32m\"\"\u001b[0m\"\n",
       "\n",
       "    span: str  # Such as: \u001b[32m\"Earth\"\u001b[0m, \u001b[32m\"Jupiter\"\u001b[0m, \u001b[32m\"Venus\"\u001b[0m, \u001b[32m\"Mercury\"\u001b[0m, \u001b[32m\"Saturn\"\u001b[0m\n",
       "\n",
       "\n",
       "@dataclass\n",
       "class \u001b[1;35mLauncher\u001b[0m\u001b[1m(\u001b[0mEntity\u001b[1m)\u001b[0m:\n",
       "    \u001b[32m\"\"\u001b[0m\"Refers to a vehicle designed primarily to transport payloads from the Earth's\n",
       "    surface to space. Launchers can carry various payloads, including satellites,\n",
       "    crewed spacecraft, and cargo, into various orbits or even beyond Earth's orbit.\n",
       "    They are usually multi-stage vehicles that use rocket engines for propulsion.\u001b[32m\"\"\u001b[0m\"\n",
       "\n",
       "    span: str  # Such as: \u001b[32m\"Sturn V\"\u001b[0m, \u001b[32m\"Atlas V\"\u001b[0m, \u001b[32m\"Soyuz\"\u001b[0m, \u001b[32m\"Ariane 5\"\u001b[0m\n",
       "\n",
       "\n",
       "# This is the text to analyze\n",
       "text = \u001b[32m\"SpaceX is colaborating with NASA in the mission to bring humans to Mars using their new Starship rocket.\"\u001b[0m\n",
       "\n",
       "# The annotation instances that take place in the text above are listed here\n",
       "result = \u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mPrivateSpaceCompany\u001b[0m\u001b[1m(\u001b[0m\u001b[33mspan\u001b[0m=\u001b[32m\"SpaceX\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPublicSpaceCompany\u001b[0m\u001b[1m(\u001b[0m\u001b[33mspan\u001b[0m=\u001b[32m\"NASA\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPlanet\u001b[0m\u001b[1m(\u001b[0m\u001b[33mspan\u001b[0m=\u001b[32m\"Mars\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mLauncher\u001b[0m\u001b[1m(\u001b[0m\u001b[33mspan\u001b[0m=\u001b[32m\"Starship\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[1m]\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(formated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6165c0d",
   "metadata": {},
   "source": [
    "## Prepare model inputs\n",
    "\n",
    "We remove everything after `result =` to run inference with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19eabf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, _ = formated_text.split(\"result =\")\n",
    "prompt = prompt + \"result =\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395394ae",
   "metadata": {},
   "source": [
    "Tokenize the input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f13c79d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = tokenizer(prompt, add_special_tokens=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef49edd",
   "metadata": {},
   "source": [
    "Remove the `eos` token from the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dfc622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input[\"input_ids\"] = model_input[\"input_ids\"][:, :-1]\n",
    "model_input[\"attention_mask\"] = model_input[\"attention_mask\"][:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6718528",
   "metadata": {},
   "source": [
    "## Run GoLLIE\n",
    "\n",
    "We generate the predictions using GoLLIE.\n",
    "\n",
    "We use `num_beams=1` and `do_sample=False` in our exmperiments. But feel free to experiment with differen decoding strategies ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6f95263",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument to exchangeDevice",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/Gollie/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Gollie/lib/python3.11/site-packages/transformers/generation/utils.py:1736\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1728\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1729\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1730\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1731\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1732\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1733\u001b[0m     )\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1736\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/Gollie/lib/python3.11/site-packages/transformers/generation/utils.py:2375\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2372\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2374\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2375\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2378\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2379\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2383\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Gollie/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Gollie/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/export/hhome/nlp2_g09/Project/GoLLIE_MED/notebooks/../src/model/patch_models/modeling_flash_llama.py:850\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, is_padded_inputs)\u001b[0m\n\u001b[1;32m    847\u001b[0m is_padded_inputs \u001b[38;5;241m=\u001b[39m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m attention_mask\u001b[38;5;241m.\u001b[39mall()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    849\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 850\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_padded_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_padded_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/Gollie/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Gollie/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/export/hhome/nlp2_g09/Project/GoLLIE_MED/notebooks/../src/model/patch_models/modeling_flash_llama.py:740\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, is_padded_inputs)\u001b[0m\n\u001b[1;32m    731\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    732\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    733\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    737\u001b[0m         is_padded_inputs,\n\u001b[1;32m    738\u001b[0m     )\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_padded_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_padded_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/Gollie/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Gollie/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/export/hhome/nlp2_g09/Project/GoLLIE_MED/notebooks/../src/model/patch_models/modeling_flash_llama.py:505\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, is_padded_inputs, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    502\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 505\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_padded_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_padded_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    514\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    516\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Gollie/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Gollie/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/export/hhome/nlp2_g09/Project/GoLLIE_MED/notebooks/../src/model/patch_models/modeling_flash_llama.py:383\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, is_padded_inputs)\u001b[0m\n\u001b[1;32m    380\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    381\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m--> 383\u001b[0m q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m kv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([k, v], \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    386\u001b[0m kv \u001b[38;5;241m=\u001b[39m repeat_kv(kv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n",
      "File \u001b[0;32m~/miniconda3/envs/Gollie/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Gollie/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/export/hhome/nlp2_g09/Project/GoLLIE_MED/notebooks/../src/model/patch_models/modeling_flash_llama.py:227\u001b[0m, in \u001b[0;36mFlashRotaryEmbedding.forward\u001b[0;34m(self, q, k, seqlen_offset)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_cos_sin_cache(q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m seqlen_offset, device\u001b[38;5;241m=\u001b[39mq\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mq\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_rotary_emb_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cos_cached\u001b[49m\u001b[43m[\u001b[49m\u001b[43mseqlen_offset\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sin_cached\u001b[49m\u001b[43m[\u001b[49m\u001b[43mseqlen_offset\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterleaved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace=True\u001b[39;49;00m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m, apply_rotary_emb_func(\n\u001b[1;32m    234\u001b[0m         k,\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cos_cached[seqlen_offset:],\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sin_cached[seqlen_offset:],\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterleaved,\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace=True\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     )\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Gollie/lib/python3.11/site-packages/flash_attn/layers/rotary.py:122\u001b[0m, in \u001b[0;36mapply_rotary_emb\u001b[0;34m(x, cos, sin, interleaved, inplace, seqlen_offsets, cu_seqlens, max_seqlen)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_rotary_emb\u001b[39m(\n\u001b[1;32m     95\u001b[0m     x,\n\u001b[1;32m     96\u001b[0m     cos,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m     max_seqlen: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    103\u001b[0m ):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m        x: (batch_size, seqlen, nheads, headdim) if cu_seqlens is None\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    Apply rotary embedding to the first rotary_dim of x.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mApplyRotaryEmb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterleaved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqlen_offsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seqlen\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Gollie/lib/python3.11/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/Gollie/lib/python3.11/site-packages/flash_attn/layers/rotary.py:48\u001b[0m, in \u001b[0;36mApplyRotaryEmb.forward\u001b[0;34m(ctx, x, cos, sin, interleaved, inplace, seqlen_offsets, cu_seqlens, max_seqlen)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     38\u001b[0m     ctx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     max_seqlen: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m ):\n\u001b[0;32m---> 48\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqlen_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseqlen_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterleaved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterleaved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(seqlen_offsets, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m     59\u001b[0m         ctx\u001b[38;5;241m.\u001b[39msave_for_backward(cos, sin, cu_seqlens)  \u001b[38;5;66;03m# Can't save int with save_for_backward\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/Gollie/lib/python3.11/site-packages/flash_attn/ops/triton/rotary.py:201\u001b[0m, in \u001b[0;36mapply_rotary\u001b[0;34m(x, cos, sin, seqlen_offsets, cu_seqlens, max_seqlen, interleaved, inplace, conjugate)\u001b[0m\n\u001b[1;32m    197\u001b[0m BLOCK_M \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interleaved \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;241m8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rotary_dim \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Need this, otherwise Triton tries to launch from cuda:0 and we get\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# ValueError: Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrotary_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# data ptrs\u001b[39;49;00m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBLOCK_M\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/Gollie/lib/python3.11/site-packages/torch/cuda/__init__.py:370\u001b[0m, in \u001b[0;36mdevice.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_idx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exchange_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument to exchangeDevice"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_ouput = model.generate(\n",
    "    **model_input.to(model.device),\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    min_new_tokens=0,\n",
    "    num_beams=1,\n",
    "    num_return_sequences=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f4a2a",
   "metadata": {},
   "source": [
    "### Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31808b61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_ouput' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y, x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mmodel_ouput\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     rich\u001b[38;5;241m.\u001b[39mprint(tokenizer\u001b[38;5;241m.\u001b[39mdecode(x,skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult = \u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_ouput' is not defined"
     ]
    }
   ],
   "source": [
    "for y, x in enumerate(model_ouput):\n",
    "    print(f\"Answer {y}\")\n",
    "    rich.print(tokenizer.decode(x,skip_special_tokens=True).split(\"result = \")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2013f",
   "metadata": {},
   "source": [
    "## Parse the output\n",
    "\n",
    "The output is a Python list of instances, we can execute it  ðŸ¤¯\n",
    "\n",
    "We define the AnnotationList class to parse the output with a single line of code. The `AnnotationList.from_output` function filters any label that we did not define (hallucinations) to prevent getting an `undefined class` error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d66fb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PrivateSpaceCompany</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">span</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'SpaceX'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PublicSpaceCompany</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">span</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'NASA'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Planet</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">span</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Mars'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Launcher</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">span</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Starship'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mPrivateSpaceCompany\u001b[0m\u001b[1m(\u001b[0m\u001b[33mspan\u001b[0m=\u001b[32m'SpaceX'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPublicSpaceCompany\u001b[0m\u001b[1m(\u001b[0m\u001b[33mspan\u001b[0m=\u001b[32m'NASA'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPlanet\u001b[0m\u001b[1m(\u001b[0m\u001b[33mspan\u001b[0m=\u001b[32m'Mars'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mLauncher\u001b[0m\u001b[1m(\u001b[0m\u001b[33mspan\u001b[0m=\u001b[32m'Starship'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = AnnotationList.from_output(\n",
    "    tokenizer.decode(model_ouput[0],skip_special_tokens=True).split(\"result = \")[-1],\n",
    "    task_module=\"guidelines\"\n",
    "    )\n",
    "rich.print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27586bd8",
   "metadata": {},
   "source": [
    "Labels are an instance of the defined classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd039309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "guidelines.PrivateSpaceCompany"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703325b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SpaceX'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].span"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c52537",
   "metadata": {},
   "source": [
    "# Evaluate the result\n",
    "\n",
    "Finally, we will evaluate the outputs from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715535b7",
   "metadata": {},
   "source": [
    "First, we define an Scorer, for Named Entity Recognition, we will use the `SpanScorer` class.\n",
    "\n",
    "We need to define the `valid_types` for the scorer, which will be the labels that we have defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e1f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tasks.utils_scorer import SpanScorer\n",
    "\n",
    "class MyEntityScorer(SpanScorer):\n",
    "    \"\"\"Compute the F1 score for Named Entity Recogtion Tasks\"\"\"\n",
    "\n",
    "    valid_types: List[Type] = ENTITY_DEFINITIONS\n",
    "\n",
    "    def __call__(self, reference: List[Entity], predictions: List[Entity]) -> Dict[str, Dict[str, float]]:\n",
    "        output = super().__call__(reference, predictions)\n",
    "        return {\"entities\": output[\"spans\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9907289",
   "metadata": {},
   "source": [
    "### Instanciate the scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = MyEntityScorer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb11ce1",
   "metadata": {},
   "source": [
    "### Compute F1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da72e844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'entities'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'precision'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'recall'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'f1-score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'class_scores'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'PrivateSpaceCompany'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'tp'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_pos'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_pre'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'precision'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'recall'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'f1-score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'PublicSpaceCompany'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'tp'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_pos'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_pre'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'precision'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'recall'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'f1-score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Planet'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'tp'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_pos'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'total_pre'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'precision'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'recall'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'f1-score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Launcher'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'tp'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_pos'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_pre'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'precision'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'recall'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'f1-score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'entities'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'precision'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "        \u001b[32m'recall'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "        \u001b[32m'f1-score'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "        \u001b[32m'class_scores'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'PrivateSpaceCompany'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'tp'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'total_pos'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'total_pre'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'precision'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "                \u001b[32m'recall'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "                \u001b[32m'f1-score'\u001b[0m: \u001b[1;36m1.0\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'PublicSpaceCompany'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'tp'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'total_pos'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'total_pre'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'precision'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "                \u001b[32m'recall'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "                \u001b[32m'f1-score'\u001b[0m: \u001b[1;36m1.0\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'Planet'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'tp'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'total_pos'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'total_pre'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'precision'\u001b[0m: \u001b[1;36m1.0\u001b[0m, \u001b[32m'recall'\u001b[0m: \u001b[1;36m1.0\u001b[0m, \u001b[32m'f1-score'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'Launcher'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'tp'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'total_pos'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'total_pre'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'precision'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "                \u001b[32m'recall'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "                \u001b[32m'f1-score'\u001b[0m: \u001b[1;36m1.0\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scorer_results = scorer(reference=[gold],predictions=[result])\n",
    "rich.print(scorer_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc27a0a2",
   "metadata": {},
   "source": [
    "GoLLIE has successfully labeled a sentence using a set of labels that were not part of the pretraining dataset ðŸŽ‰ðŸŽ‰ðŸŽ‰\n",
    "\n",
    "GoLLIE will perform well on labels with well-defined and clearly bounded guidelines. \n",
    "\n",
    "Please share your cool experiments with us; we'd love to see what everyone is doing with GoLLIE!\n",
    "- [@iker_garciaf](https://twitter.com/iker_garciaf)\n",
    "- [@osainz59](https://twitter.com/osainz59)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gollie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
