{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoLLIE for Medical Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import requeriments\n",
    "See the requeriments.txt file in the main directory to install the required dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") # Add the GoLLIE base directory to sys path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ndelafuente/miniconda3/envs/gollie_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import rich \n",
    "import logging\n",
    "from src.model.load_model import load_model\n",
    "import black\n",
    "import inspect\n",
    "from jinja2 import Template as jinja2Template\n",
    "import tempfile\n",
    "from src.tasks.utils_typing import AnnotationList\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from typing import Dict, List, Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GoLLIE\n",
    "\n",
    "Load GOLLIE-7B from the huggingface-hub.\n",
    "Use the AutoModelForCausalLM.from_pretrained function if you prefer it. However, creators provide a handy load_model function with many functionalities already implemented that will assist you in reproducing our results.\n",
    "\n",
    "Please note that setting use_flash_attention=True is mandatory. Our flash attention implementation has small numerical differences compared to the attention implementation in Huggingface. Using use_flash_attention=False will result in the model producing inferior results. Flash attention requires an available CUDA GPU. Running GOLLIE pre-trained models on a CPU is not supported.\n",
    "\n",
    "- Set force_auto_device_map=True to automatically load the model on available GPUs.\n",
    "- Set quantization=4 if the model doesn't fit in your GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading model model from HiTZ/GoLLIE-7B\n",
      "WARNING:root:Using auto device map, we will split the model across GPUs and CPU to fit the model in memory.\n",
      "INFO:root:We will load the model using the following device map: auto and max_memory: None\n",
      "/home/ndelafuente/miniconda3/envs/gollie_env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "INFO:root:Loading model with dtype: torch.bfloat16\n",
      "WARNING:root:Model HiTZ/GoLLIE-7B is an decoder-only model. We will load it as a CausalLM model.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Please install Flash Attention: `pip install flash-attn --no-build-isolation`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/GoLLIE_MED/notebooks/../src/model/patch_models/modeling_flash_llama.py:51\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbert_padding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_input, unpad_input\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attn_interface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     53\u001b[0m         flash_attn_kvpacked_func,\n\u001b[1;32m     54\u001b[0m         flash_attn_varlen_kvpacked_func,\n\u001b[1;32m     55\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flash_attn'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Use the custom load_model for loading GoLLIE\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_weights_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHiTZ/GoLLIE-7B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_lora\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_auto_device_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_flash_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbfloat16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GoLLIE_MED/notebooks/../src/model/load_model.py:369\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(inference, model_weights_name_or_path, quantization, use_lora, lora_weights_name_or_path, lora_target_modules, lora_r, lora_alpha, lora_dropout, torch_dtype, force_auto_device_map, use_gradient_checkpointing, trust_remote_code, use_flash_attention, use_better_transformer, fsdp_training, max_memory_MB)\u001b[0m\n\u001b[1;32m    364\u001b[0m logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_weights_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is an decoder-only model. We will load it as a CausalLM model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mmodel_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_flash_attention:\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatch_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_flash_llama\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaForCausalLM \u001b[38;5;28;01mas\u001b[39;00m LlamaForCausalLMFlash\n\u001b[1;32m    371\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Flash Attention for LLaMA model.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    372\u001b[0m     load_fn \u001b[38;5;241m=\u001b[39m LlamaForCausalLMFlash\n",
      "File \u001b[0;32m~/Desktop/GoLLIE_MED/notebooks/../src/model/patch_models/modeling_flash_llama.py:61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     flash_attn_v2_installed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install Flash Attention: `pip install flash-attn --no-build-isolation`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflash_attn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrotary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_rotary_emb_func\n",
      "\u001b[0;31mImportError\u001b[0m: Please install Flash Attention: `pip install flash-attn --no-build-isolation`"
     ]
    }
   ],
   "source": [
    "#Use the custom load_model for loading GoLLIE\n",
    "model, tokenizer = load_model(\n",
    "    inference=True,\n",
    "    model_weights_name_or_path=\"HiTZ/GoLLIE-7B\",\n",
    "    quantization=None,\n",
    "    use_lora=False,\n",
    "    force_auto_device_map=True,\n",
    "    use_flash_attention=True,\n",
    "    torch_dtype=\"bfloat16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the guidelines\n",
    "\n",
    "First, we will define the labels and guidelines for the task. We will represent them as Python classes.\n",
    "\n",
    "The following guidelines have been defined for this example. They were not part of the pre-training dataset. Therefore, we will run GOLLIE in zero-shot settings using unseen labels.\n",
    "\n",
    "We will use the `Generic` class, which is a versatile class that allows for the implementation of any task you want. However, since the model has never seen the Generic label during training, we will rename it to Template, which is recognized by the model (as it was used in the Tacred dataset).\n",
    "\n",
    "We will define several classes: `Illness`, `Medication`, `PatientData`, `HospitalizationData`. Each class will have a definition and a set of slots that the model needs to fill. Each slot also requires a type definition and a short description, which can include examples. For instance, for the `Illness` class, we define three slots:\n",
    "\n",
    "- The `mention`, which will be the name of the Ilness of the patient and should be a string.\n",
    "- The `treatment` which will be a list of treatments or interventions used to manage the illness. \n",
    "- The `symptoms`, which is defined as a list of symptoms. Therefore, GoLLIE will fill this slot with a list of strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from src.tasks.utils_typing import dataclass\n",
    "from src.tasks.utils_typing import Generic as Template\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Entity definitions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Medication(Template):\n",
    "    \"\"\"Refers to a drug or substance used to diagnose, cure, treat, or prevent disease.\n",
    "    Medications can be administered in various forms and dosages and are crucial \n",
    "    in managing patient health conditions. They can be classified based on their \n",
    "    therapeutic use, mechanism of action, or chemical characteristics.\"\"\"\n",
    "    \n",
    "    mention: str\n",
    "    \"\"\"\n",
    "    The name of the medication.\n",
    "    Such as: \"Aspirina\", \"Ibuprofeno\", \"Aspirina\".\n",
    "    \"\"\"\n",
    "    dosage: str # The amount and frequency at which the medication is prescribed. Such as: \"100 mg al día\", \"200 mg dos veces al día\"\n",
    "    route: str # The method of administration for the medication. Such as: \"oral\", \"intravenoso\", \"tópico\"\n",
    "    purpose: List[str]  # List of reasons or conditions for which the medication is prescribed. Such as: [\"dolor\", \"control de azúcar en la sangre\", \"inflamación\"]\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class Ilness(Template):\n",
    "    \"\"\"Refers to a health condition or disease that affects the body's normal functioning.\n",
    "    Illnesses can be caused by various factors, such as infections, genetic disorders,\n",
    "    lifestyle choices, or environmental factors. They can affect different body systems\n",
    "    and have varying degrees of severity.\"\"\"\n",
    "    \n",
    "    mention: str\n",
    "    \"\"\"\n",
    "    The name of the illness or health condition.\n",
    "    Such as: \"diabetes\", \"cáncer\", \"hipertensión\".\n",
    "    \"\"\"\n",
    "    symptoms: List[str] # List of signs or symptoms associated with the illness. Such as: [\"dolor de cabeza\", \"fatiga\", \"fiebre\"]\n",
    "    treatment: List[str] # List of treatments or interventions used to manage the illness. Such as: [\"medicamentos\", \"cirugía\", \"terapia física\"]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HospitalizationData:\n",
    "    \"\"\"Refers to information related to a patient's hospitalization, including the\n",
    "    admission date, discharge date, and reason for hospitalization. Hospitalization\n",
    "    data is essential for tracking patient health status, treatment progress, and\n",
    "    healthcare resource utilization.\"\"\"\n",
    "    \n",
    "    admission_date: str #The date on which the patient was admitted to the hospital.\n",
    "    discharge_date: str #The date on which the patient was discharged from the hospital.\n",
    "    reason: str #the reason or cause for the patient's hospitalization.\n",
    "    \n",
    "    \n",
    "@dataclass\n",
    "class PatientData:\n",
    "    \"\"\"Refers to information related to a patient's medical history, including\n",
    "    name, age or urgency. Patient data is essential for healthcare providers \n",
    "    to provide appropriate care and make informed decisions about patient management.\"\"\"\n",
    "    \n",
    "    name: str #The name of the patient.\n",
    "    age: int #The age of the patient.\n",
    "    urgency: str #The urgency level of the patient's condition.\n",
    "    \n",
    "    \n",
    "    \n",
    "ENTITY_DEFINITIONS: List[Template] = [\n",
    "    Medication,\n",
    "    Ilness,\n",
    "    HospitalizationData,\n",
    "]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cell_txt = In[-1] #In needs to be imported from IPython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class Launcher(Template):\n",
    "    \"\"\"Refers to a vehicle designed primarily to transport payloads from the Earth's \n",
    "    surface to space. Launchers can carry various payloads, including satellites, \n",
    "    crewed spacecraft, and cargo, into various orbits or even beyond Earth's orbit. \n",
    "    They are usually multi-stage vehicles that use rocket engines for propulsion.\"\"\"\n",
    "\n",
    "    mention: str  \n",
    "    \"\"\"\n",
    "    The name of the launcher vehicle. \n",
    "    Such as: \"Sturn V\", \"Atlas V\", \"Soyuz\", \"Ariane 5\"\n",
    "    \"\"\"\n",
    "    space_company: str # The company that operates the launcher. Such as: \"Blue origin\", \"ESA\", \"Boeing\", \"ISRO\", \"Northrop Grumman\", \"Arianespace\"\n",
    "    crew: List[str] # Names of the crew members boarding the Launcher. Such as: \"Neil Armstrong\", \"Michael Collins\", \"Buzz Aldrin\"\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class Mission(Template):\n",
    "    \"\"\"Any planned or accomplished journey beyond Earth's atmosphere with specific objectives, \n",
    "    either crewed or uncrewed. It includes missions to satellites, the International \n",
    "    Space Station (ISS), other celestial bodies, and deep space.\"\"\"\n",
    "    \n",
    "    mention: str\n",
    "    \"\"\"\n",
    "    The name of the mission. \n",
    "    Such as: \"Apollo 11\", \"Artemis\", \"Mercury\"\n",
    "    \"\"\"\n",
    "    date: str # The start date of the mission\n",
    "    departure: str # The place from which the vehicle will be launched. Such as: \"Florida\", \"Houston\", \"French Guiana\"\n",
    "    destination: str # The place or planet to which the launcher will be sent. Such as \"Moon\", \"low-orbit\", \"Saturn\"\n",
    "\n",
    "\n",
    "ENTITY_DEFINITIONS: List[Template] = [\n",
    "    Launcher,\n",
    "    Mission,\n",
    "]\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    cell_txt = In[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
