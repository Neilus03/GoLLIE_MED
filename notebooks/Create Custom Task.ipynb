{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2afe3b",
   "metadata": {},
   "source": [
    "<img src=\"../assets/CoLLIE_blue.png\" alt=\"GoLLIE\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003b73a2",
   "metadata": {},
   "source": [
    "# Custom Tasks with GoLLIE\n",
    "\n",
    "This notebook is an example of how to run Custom Tasks with GoLLIE. This notebook covers:\n",
    "\n",
    "- How to define the guidelines for a task\n",
    "- How to load GoLLIE\n",
    "- How to generate model inputs\n",
    "- How to parse the output\n",
    "- How to implement a scorer and evaluate the output\n",
    "\n",
    "You can modify this notebook to run any task task you want "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b015c64",
   "metadata": {},
   "source": [
    "### Import requeriments\n",
    "\n",
    "See the requeriments.txt file in the main directory to install the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed51491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\") # Add the GoLLIE base directory to sys path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28ff498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich\n",
    "import logging\n",
    "from src.model.load_model import load_model\n",
    "import black\n",
    "import inspect\n",
    "from jinja2 import Template as jinja2Template\n",
    "import tempfile\n",
    "from src.tasks.utils_typing import AnnotationList\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from typing import Dict, List, Type\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004626bb",
   "metadata": {},
   "source": [
    "## Load GoLLIE\n",
    "\n",
    "We will load GOLLIE-7B from the huggingface-hub.\n",
    "You can use the function AutoModelForCausalLM.from_pretrained if you prefer it. However, we provide a handy load_model function with many functionalities already implemented that will assist you in reproducing our results.\n",
    "\n",
    "Please note that setting use_flash_attention=True is mandatory. Our flash attention implementation has small numerical differences compared to the attention implementation in Huggingface. Using use_flash_attention=False will result in the model producing inferior results. Flash attention requires an available CUDA GPU. Running GOLLIE pre-trained models on a CPU is not supported. We plan to address this in future releases.\n",
    "\n",
    "- Set force_auto_device_map=True to automatically load the model on available GPUs.\n",
    "- Set quantization=4 if the model doesn't fit in your GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb841c40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading model model from HiTZ/GoLLIE-7B\n",
      "WARNING:root:Using auto device map, we will split the model across GPUs and CPU to fit the model in memory.\n",
      "INFO:root:We will load the model using the following device map: auto and max_memory: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1980fa22398942a8aedd5036f4a4403b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/748 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e7d43e23884750b4d6e8cc30dd06e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/945 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2807122457d43c4a1d0fe92db12fece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0219b4b26bc4d83a98634e5d635f0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad893165b504bccaba3956534a60ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/536 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading model with dtype: torch.bfloat16\n",
      "WARNING:root:Model HiTZ/GoLLIE-7B is an decoder-only model. We will load it as a CausalLM model.\n",
      "WARNING:root:Using Flash Attention for LLaMA model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Flash Attention installed\n",
      ">>>> Flash RoPE installed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869ae0e048244ce2a8fbdf3ace167cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f443e05c4b4d415fafc952f6f0380bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94534982c44c406da8830422e61da85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a163a45eb2c7416aa1bdc3d74b289116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44af7ec0807640c48f9c2120a55c66ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed0b3241512494c9805b9ba53936bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Model dtype: torch.bfloat16\n",
      "INFO:root:Total model memory footprint: 13477.101762 MB\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(\n",
    "    inference=True,\n",
    "    model_weights_name_or_path=\"HiTZ/GoLLIE-7B\",\n",
    "    quantization=None,\n",
    "    use_lora=False,\n",
    "    force_auto_device_map=True,\n",
    "    use_flash_attention=True,\n",
    "    torch_dtype=\"bfloat16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a662bf3",
   "metadata": {},
   "source": [
    "## Define the guidelines\n",
    "\n",
    "First, we will define the labels and guidelines for the task. We will represent them as Python classes.\n",
    "\n",
    "The following guidelines have been defined for this example. They were not part of the pre-training dataset. Therefore, we will run GOLLIE in zero-shot settings using unseen labels.\n",
    "\n",
    "We will use the `Generic` class, which is a versatile class that allows for the implementation of any task you want. However, since the model has never seen the Generic label during training, we will rename it to Template, which is recognized by the model (as it was used in the Tacred dataset).\n",
    "\n",
    "We will define two classes: `Launcher` and `Mission`. Each class will have a definition and a set of slots that the model needs to fill. Each slot also requires a type definition and a short description, which can include examples. For instance, for the `Launcher` class, we define three slots:\n",
    "\n",
    "- The `mention`, which will be the name of the Launcher vehicle and should be a string.\n",
    "- The `space_company` that operated the vehicle, which will also be a string.\n",
    "- The `crew`, which is defined as a list of astronauts. Therefore, GoLLIE will fill this slot with a list of strings.\n",
    "\n",
    "ðŸ’¡ Be creative and try to define your own guidelines to test GoLLIE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd5d262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from src.tasks.utils_typing import dataclass\n",
    "from src.tasks.utils_typing import Generic as Template\n",
    "\n",
    "\"\"\"\n",
    "Entity definitions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Launcher(Template):\n",
    "    \"\"\"Refers to a vehicle designed primarily to transport payloads from the Earth's \n",
    "    surface to space. Launchers can carry various payloads, including satellites, \n",
    "    crewed spacecraft, and cargo, into various orbits or even beyond Earth's orbit. \n",
    "    They are usually multi-stage vehicles that use rocket engines for propulsion.\"\"\"\n",
    "\n",
    "    mention: str  \n",
    "    \"\"\"\n",
    "    The name of the launcher vehicle. \n",
    "    Such as: \"Sturn V\", \"Atlas V\", \"Soyuz\", \"Ariane 5\"\n",
    "    \"\"\"\n",
    "    space_company: str # The company that operates the launcher. Such as: \"Blue origin\", \"ESA\", \"Boeing\", \"ISRO\", \"Northrop Grumman\", \"Arianespace\"\n",
    "    crew: List[str] # Names of the crew members boarding the Launcher. Such as: \"Neil Armstrong\", \"Michael Collins\", \"Buzz Aldrin\"\n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class Mission(Template):\n",
    "    \"\"\"Any planned or accomplished journey beyond Earth's atmosphere with specific objectives, \n",
    "    either crewed or uncrewed. It includes missions to satellites, the International \n",
    "    Space Station (ISS), other celestial bodies, and deep space.\"\"\"\n",
    "    \n",
    "    mention: str\n",
    "    \"\"\"\n",
    "    The name of the mission. \n",
    "    Such as: \"Apollo 11\", \"Artemis\", \"Mercury\"\n",
    "    \"\"\"\n",
    "    date: str # The start date of the mission\n",
    "    departure: str # The place from which the vehicle will be launched. Such as: \"Florida\", \"Houston\", \"French Guiana\"\n",
    "    destination: str # The place or planet to which the launcher will be sent. Such as \"Moon\", \"low-orbit\", \"Saturn\"\n",
    "\n",
    "\n",
    "ENTITY_DEFINITIONS: List[Template] = [\n",
    "    Launcher,\n",
    "    Mission,\n",
    "]\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    cell_txt = In[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f8ef55",
   "metadata": {},
   "source": [
    "### Print the guidelines to guidelines.py\n",
    "\n",
    "Due to IPython limitations, we must write the content of the previous cell to a file and then import the content from that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4736a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"guidelines.py\",\"w\",encoding=\"utf8\") as python_guidelines:\n",
    "    print(cell_txt,file=python_guidelines)\n",
    "\n",
    "from guidelines import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac3db6f",
   "metadata": {},
   "source": [
    "We use inspect.getsource to get the guidelines as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89454475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@dataclass\\nclass Launcher(Template):\\n    \"\"\"Refers to a vehicle designed primarily to transport payloads from the Earth\\'s \\n    surface to space. Launchers can carry various payloads, including satellites, \\n    crewed spacecraft, and cargo, into various orbits or even beyond Earth\\'s orbit. \\n    They are usually multi-stage vehicles that use rocket engines for propulsion.\"\"\"\\n\\n    mention: str  \\n    \"\"\"\\n    The name of the launcher vehicle. \\n    Such as: \"Sturn V\", \"Atlas V\", \"Soyuz\", \"Ariane 5\"\\n    \"\"\"\\n    space_company: str # The company that operates the launcher. Such as: \"Blue origin\", \"ESA\", \"Boeing\", \"ISRO\", \"Northrop Grumman\", \"Arianespace\"\\n    crew: List[str] # Names of the crew members boarding the Launcher. Such as: \"Neil Armstrong\", \"Michael Collins\", \"Buzz Aldrin\"\\n',\n",
       " '@dataclass\\nclass Mission(Template):\\n    \"\"\"Any planned or accomplished journey beyond Earth\\'s atmosphere with specific objectives, \\n    either crewed or uncrewed. It includes missions to satellites, the International \\n    Space Station (ISS), other celestial bodies, and deep space.\"\"\"\\n    \\n    mention: str\\n    \"\"\"\\n    The name of the mission. \\n    Such as: \"Apollo 11\", \"Artemis\", \"Mercury\"\\n    \"\"\"\\n    date: str # The start date of the mission\\n    departure: str # The place from which the vehicle will be launched. Such as: \"Florida\", \"Houston\", \"French Guiana\"\\n    destination: str # The place or planet to which the launcher will be sent. Such as \"Moon\", \"low-orbit\", \"Saturn\"\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guidelines = [inspect.getsource(definition) for definition in ENTITY_DEFINITIONS]\n",
    "guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bd26b7",
   "metadata": {},
   "source": [
    "## Define input sentence\n",
    "\n",
    "Here we define the input sentence and the gold labels.\n",
    "\n",
    "You can define and empy list as gold labels if you don't have gold annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfb92535",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The Ares 3 mission to Mars is scheduled for 2032. The Starship rocket build by SpaceX will take off from Boca Chica, carrying the astronauts Max Rutherford, Elena Soto, and Jake Martinez.\"\n",
    "gold = [\n",
    "    Launcher(mention=\"Starship\",space_company=\"SpaceX\",crew=[\"Max Rutherford\",\"Elena Soto\",\"Jake Martinez\"]),\n",
    "    Mission(mention=\"Ares 3\",date=\"2032\",departure=\"Boca Chica\",destination=\"Mars\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90501322",
   "metadata": {},
   "source": [
    "## Filling a template\n",
    "\n",
    "We need to define a template. For this task, we will include only the class definitions and the text to be annotated. However, you can design different templates to incorporate more information (for example, event triggers, as demonstrated in the Event Extraction notebook).\n",
    "\n",
    "We will use Jinja templates, which are easy to implement and exceptionally fast. For more information, visit: https://jinja.palletsprojects.com/en/3.1.x/api/#high-level-api.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b365413",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_txt =(\n",
    "\"\"\"# The following lines describe the task definition\n",
    "{%- for definition in guidelines %}\n",
    "{{ definition }}\n",
    "{%- endfor %}\n",
    "\n",
    "# This is the text to analyze\n",
    "text = {{ text.__repr__() }}\n",
    "\n",
    "# The annotation instances that take place in the text above are listed here\n",
    "result = [\n",
    "{%- for ann in annotations %}\n",
    "    {{ ann }},\n",
    "{%- endfor %}\n",
    "]\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f54034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = jinja2Template(template_txt)\n",
    "# Fill the template\n",
    "formated_text = template.render(guidelines=guidelines, text=text, annotations=gold, gold=gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886dbc0c",
   "metadata": {},
   "source": [
    "### Black Code Formatter\n",
    "\n",
    "We use the Black Code Formatter to automatically unify all the prompts to the same format. \n",
    "\n",
    "https://github.com/psf/black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8994924",
   "metadata": {},
   "outputs": [],
   "source": [
    "black_mode = black.Mode()\n",
    "formated_text = black.format_str(formated_text, mode=black_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aa5c0e",
   "metadata": {},
   "source": [
    "### Print the filled and formatted template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa5f3106",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"># The following lines describe the task definition\n",
       "@dataclass\n",
       "class <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Launcher</span><span style=\"font-weight: bold\">(</span>Template<span style=\"font-weight: bold\">)</span>:\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"Refers to a vehicle designed primarily to transport payloads from the Earth's\n",
       "    surface to space. Launchers can carry various payloads, including satellites,\n",
       "    crewed spacecraft, and cargo, into various orbits or even beyond Earth's orbit.\n",
       "    They are usually multi-stage vehicles that use rocket engines for propulsion.<span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"\n",
       "\n",
       "    mention: str\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"\n",
       "    The name of the launcher vehicle. \n",
       "    Such as: <span style=\"color: #008000; text-decoration-color: #008000\">\"Sturn V\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Atlas V\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Soyuz\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Ariane 5\"</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"\n",
       "    space_company: str  # The company that operates the launcher. Such as: <span style=\"color: #008000; text-decoration-color: #008000\">\"Blue origin\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"ESA\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Boeing\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"ISRO\"</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"Northrop Grumman\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Arianespace\"</span>\n",
       "    crew: List<span style=\"font-weight: bold\">[</span>\n",
       "        str\n",
       "    <span style=\"font-weight: bold\">]</span>  # Names of the crew members boarding the Launcher. Such as: <span style=\"color: #008000; text-decoration-color: #008000\">\"Neil Armstrong\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Michael Collins\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Buzz </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Aldrin\"</span>\n",
       "\n",
       "\n",
       "@dataclass\n",
       "class <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Mission</span><span style=\"font-weight: bold\">(</span>Template<span style=\"font-weight: bold\">)</span>:\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"Any planned or accomplished journey beyond Earth's atmosphere with specific objectives,\n",
       "    either crewed or uncrewed. It includes missions to satellites, the International\n",
       "    Space Station <span style=\"font-weight: bold\">(</span>ISS<span style=\"font-weight: bold\">)</span>, other celestial bodies, and deep space.<span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"\n",
       "\n",
       "    mention: str\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"\n",
       "    The name of the mission. \n",
       "    Such as: <span style=\"color: #008000; text-decoration-color: #008000\">\"Apollo 11\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Artemis\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Mercury\"</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">\"\"</span>\"\n",
       "    date: str  # The start date of the mission\n",
       "    departure: str  # The place from which the vehicle will be launched. Such as: <span style=\"color: #008000; text-decoration-color: #008000\">\"Florida\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Houston\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"French </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Guiana\"</span>\n",
       "    destination: str  # The place or planet to which the launcher will be sent. Such as <span style=\"color: #008000; text-decoration-color: #008000\">\"Moon\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"low-orbit\"</span>, \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\"Saturn\"</span>\n",
       "\n",
       "\n",
       "# This is the text to analyze\n",
       "text = <span style=\"color: #008000; text-decoration-color: #008000\">\"The Ares 3 mission to Mars is scheduled for 2032. The Starship rocket build by SpaceX will take off from </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">Boca Chica, carrying the astronauts Max Rutherford, Elena Soto, and Jake Martinez.\"</span>\n",
       "\n",
       "# The annotation instances that take place in the text above are listed here\n",
       "result = <span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Launcher</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">mention</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Starship\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">space_company</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"SpaceX\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">crew</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Max Rutherford\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Elena Soto\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Jake Martinez\"</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Mission</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">mention</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Ares 3\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">date</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"2032\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">departure</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Boca Chica\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">destination</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Mars\"</span><span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "# The following lines describe the task definition\n",
       "@dataclass\n",
       "class \u001b[1;35mLauncher\u001b[0m\u001b[1m(\u001b[0mTemplate\u001b[1m)\u001b[0m:\n",
       "    \u001b[32m\"\"\u001b[0m\"Refers to a vehicle designed primarily to transport payloads from the Earth's\n",
       "    surface to space. Launchers can carry various payloads, including satellites,\n",
       "    crewed spacecraft, and cargo, into various orbits or even beyond Earth's orbit.\n",
       "    They are usually multi-stage vehicles that use rocket engines for propulsion.\u001b[32m\"\"\u001b[0m\"\n",
       "\n",
       "    mention: str\n",
       "    \u001b[32m\"\"\u001b[0m\"\n",
       "    The name of the launcher vehicle. \n",
       "    Such as: \u001b[32m\"Sturn V\"\u001b[0m, \u001b[32m\"Atlas V\"\u001b[0m, \u001b[32m\"Soyuz\"\u001b[0m, \u001b[32m\"Ariane 5\"\u001b[0m\n",
       "    \u001b[32m\"\"\u001b[0m\"\n",
       "    space_company: str  # The company that operates the launcher. Such as: \u001b[32m\"Blue origin\"\u001b[0m, \u001b[32m\"ESA\"\u001b[0m, \u001b[32m\"Boeing\"\u001b[0m, \u001b[32m\"ISRO\"\u001b[0m, \n",
       "\u001b[32m\"Northrop Grumman\"\u001b[0m, \u001b[32m\"Arianespace\"\u001b[0m\n",
       "    crew: List\u001b[1m[\u001b[0m\n",
       "        str\n",
       "    \u001b[1m]\u001b[0m  # Names of the crew members boarding the Launcher. Such as: \u001b[32m\"Neil Armstrong\"\u001b[0m, \u001b[32m\"Michael Collins\"\u001b[0m, \u001b[32m\"Buzz \u001b[0m\n",
       "\u001b[32mAldrin\"\u001b[0m\n",
       "\n",
       "\n",
       "@dataclass\n",
       "class \u001b[1;35mMission\u001b[0m\u001b[1m(\u001b[0mTemplate\u001b[1m)\u001b[0m:\n",
       "    \u001b[32m\"\"\u001b[0m\"Any planned or accomplished journey beyond Earth's atmosphere with specific objectives,\n",
       "    either crewed or uncrewed. It includes missions to satellites, the International\n",
       "    Space Station \u001b[1m(\u001b[0mISS\u001b[1m)\u001b[0m, other celestial bodies, and deep space.\u001b[32m\"\"\u001b[0m\"\n",
       "\n",
       "    mention: str\n",
       "    \u001b[32m\"\"\u001b[0m\"\n",
       "    The name of the mission. \n",
       "    Such as: \u001b[32m\"Apollo 11\"\u001b[0m, \u001b[32m\"Artemis\"\u001b[0m, \u001b[32m\"Mercury\"\u001b[0m\n",
       "    \u001b[32m\"\"\u001b[0m\"\n",
       "    date: str  # The start date of the mission\n",
       "    departure: str  # The place from which the vehicle will be launched. Such as: \u001b[32m\"Florida\"\u001b[0m, \u001b[32m\"Houston\"\u001b[0m, \u001b[32m\"French \u001b[0m\n",
       "\u001b[32mGuiana\"\u001b[0m\n",
       "    destination: str  # The place or planet to which the launcher will be sent. Such as \u001b[32m\"Moon\"\u001b[0m, \u001b[32m\"low-orbit\"\u001b[0m, \n",
       "\u001b[32m\"Saturn\"\u001b[0m\n",
       "\n",
       "\n",
       "# This is the text to analyze\n",
       "text = \u001b[32m\"The Ares 3 mission to Mars is scheduled for 2032. The Starship rocket build by SpaceX will take off from \u001b[0m\n",
       "\u001b[32mBoca Chica, carrying the astronauts Max Rutherford, Elena Soto, and Jake Martinez.\"\u001b[0m\n",
       "\n",
       "# The annotation instances that take place in the text above are listed here\n",
       "result = \u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mLauncher\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmention\u001b[0m=\u001b[32m\"Starship\"\u001b[0m,\n",
       "        \u001b[33mspace_company\u001b[0m=\u001b[32m\"SpaceX\"\u001b[0m,\n",
       "        \u001b[33mcrew\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m\"Max Rutherford\"\u001b[0m, \u001b[32m\"Elena Soto\"\u001b[0m, \u001b[32m\"Jake Martinez\"\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mMission\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmention\u001b[0m=\u001b[32m\"Ares\u001b[0m\u001b[32m 3\"\u001b[0m, \u001b[33mdate\u001b[0m=\u001b[32m\"2032\"\u001b[0m, \u001b[33mdeparture\u001b[0m=\u001b[32m\"Boca\u001b[0m\u001b[32m Chica\"\u001b[0m, \u001b[33mdestination\u001b[0m=\u001b[32m\"Mars\"\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[1m]\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(formated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6165c0d",
   "metadata": {},
   "source": [
    "## Prepare model inputs\n",
    "\n",
    "We remove everything after `result =` to run inference with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19eabf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, _ = formated_text.split(\"result =\")\n",
    "prompt = prompt + \"result =\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0620be2",
   "metadata": {},
   "source": [
    "Tokenize the input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f13c79d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = tokenizer(prompt, add_special_tokens=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff91970",
   "metadata": {},
   "source": [
    "Remove the `eos` token from the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee9d69ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input[\"input_ids\"] = model_input[\"input_ids\"][:, :-1]\n",
    "model_input[\"attention_mask\"] = model_input[\"attention_mask\"][:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6718528",
   "metadata": {},
   "source": [
    "## Run GoLLIE\n",
    "\n",
    "We generate the predictions using GoLLIE.\n",
    "\n",
    "We use `num_beams=1` and `do_sample=False` in our exmperiments. But feel free to experiment with differen decoding strategies ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6f95263",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument to exchangeDevice",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:3\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1576\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assisted_decoding(\n\u001b[1;32m   1560\u001b[0m         input_ids,\n\u001b[1;32m   1561\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1573\u001b[0m     )\n\u001b[1;32m   1574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1575\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1576\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_greedy_search(\n\u001b[1;32m   1577\u001b[0m         input_ids,\n\u001b[1;32m   1578\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   1579\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   1580\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[1;32m   1581\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[1;32m   1582\u001b[0m         output_logits\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_logits,\n\u001b[1;32m   1583\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m   1584\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   1585\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   1586\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1587\u001b[0m     )\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:2494\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2491\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2494\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\n\u001b[1;32m   2495\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[1;32m   2496\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2497\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   2498\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   2499\u001b[0m )\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2502\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/export/hhome/nlp2_g09/Project/GoLLIE_MED/notebooks/../src/model/patch_models/modeling_flash_llama.py:850\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, is_padded_inputs)\u001b[0m\n\u001b[1;32m    847\u001b[0m is_padded_inputs \u001b[38;5;241m=\u001b[39m (attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m attention_mask\u001b[38;5;241m.\u001b[39mall()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    849\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 850\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    851\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    852\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    853\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    854\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    855\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    856\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    857\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    858\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    859\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    860\u001b[0m     is_padded_inputs\u001b[38;5;241m=\u001b[39mis_padded_inputs,\n\u001b[1;32m    861\u001b[0m )\n\u001b[1;32m    863\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/export/hhome/nlp2_g09/Project/GoLLIE_MED/notebooks/../src/model/patch_models/modeling_flash_llama.py:740\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, is_padded_inputs)\u001b[0m\n\u001b[1;32m    731\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    732\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    733\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    737\u001b[0m         is_padded_inputs,\n\u001b[1;32m    738\u001b[0m     )\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 740\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    741\u001b[0m         hidden_states,\n\u001b[1;32m    742\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    743\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    744\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    745\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    746\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    747\u001b[0m         is_padded_inputs\u001b[38;5;241m=\u001b[39mis_padded_inputs,\n\u001b[1;32m    748\u001b[0m     )\n\u001b[1;32m    750\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/export/hhome/nlp2_g09/Project/GoLLIE_MED/notebooks/../src/model/patch_models/modeling_flash_llama.py:505\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, is_padded_inputs, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    502\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 505\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    506\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    507\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    508\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    509\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    510\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    511\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    512\u001b[0m     is_padded_inputs\u001b[38;5;241m=\u001b[39mis_padded_inputs,\n\u001b[1;32m    513\u001b[0m )\n\u001b[1;32m    514\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    516\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/export/hhome/nlp2_g09/Project/GoLLIE_MED/notebooks/../src/model/patch_models/modeling_flash_llama.py:383\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, is_padded_inputs)\u001b[0m\n\u001b[1;32m    380\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    381\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m--> 383\u001b[0m q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(q, k, past_len)\n\u001b[1;32m    385\u001b[0m kv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([k, v], \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    386\u001b[0m kv \u001b[38;5;241m=\u001b[39m repeat_kv(kv, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/export/hhome/nlp2_g09/Project/GoLLIE_MED/notebooks/../src/model/patch_models/modeling_flash_llama.py:227\u001b[0m, in \u001b[0;36mFlashRotaryEmbedding.forward\u001b[0;34m(self, q, k, seqlen_offset)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_cos_sin_cache(q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m seqlen_offset, device\u001b[38;5;241m=\u001b[39mq\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mq\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m apply_rotary_emb_func(\n\u001b[1;32m    228\u001b[0m         q,\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cos_cached[seqlen_offset:],\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sin_cached[seqlen_offset:],\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterleaved,\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace=True\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     ), apply_rotary_emb_func(\n\u001b[1;32m    234\u001b[0m         k,\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cos_cached[seqlen_offset:],\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sin_cached[seqlen_offset:],\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterleaved,\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace=True\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     )\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/flash_attn/layers/rotary.py:122\u001b[0m, in \u001b[0;36mapply_rotary_emb\u001b[0;34m(x, cos, sin, interleaved, inplace, seqlen_offsets, cu_seqlens, max_seqlen)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_rotary_emb\u001b[39m(\n\u001b[1;32m     95\u001b[0m     x,\n\u001b[1;32m     96\u001b[0m     cos,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m     max_seqlen: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    103\u001b[0m ):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m        x: (batch_size, seqlen, nheads, headdim) if cu_seqlens is None\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    Apply rotary embedding to the first rotary_dim of x.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ApplyRotaryEmb\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    123\u001b[0m         x, cos, sin, interleaved, inplace, seqlen_offsets, cu_seqlens, max_seqlen\n\u001b[1;32m    124\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/flash_attn/layers/rotary.py:48\u001b[0m, in \u001b[0;36mApplyRotaryEmb.forward\u001b[0;34m(ctx, x, cos, sin, interleaved, inplace, seqlen_offsets, cu_seqlens, max_seqlen)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     38\u001b[0m     ctx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     max_seqlen: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     47\u001b[0m ):\n\u001b[0;32m---> 48\u001b[0m     out \u001b[38;5;241m=\u001b[39m apply_rotary(\n\u001b[1;32m     49\u001b[0m         x,\n\u001b[1;32m     50\u001b[0m         cos,\n\u001b[1;32m     51\u001b[0m         sin,\n\u001b[1;32m     52\u001b[0m         seqlen_offsets\u001b[38;5;241m=\u001b[39mseqlen_offsets,\n\u001b[1;32m     53\u001b[0m         cu_seqlens\u001b[38;5;241m=\u001b[39mcu_seqlens,\n\u001b[1;32m     54\u001b[0m         max_seqlen\u001b[38;5;241m=\u001b[39mmax_seqlen,\n\u001b[1;32m     55\u001b[0m         interleaved\u001b[38;5;241m=\u001b[39minterleaved,\n\u001b[1;32m     56\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(seqlen_offsets, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m     59\u001b[0m         ctx\u001b[38;5;241m.\u001b[39msave_for_backward(cos, sin, cu_seqlens)  \u001b[38;5;66;03m# Can't save int with save_for_backward\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/flash_attn/ops/triton/rotary.py:201\u001b[0m, in \u001b[0;36mapply_rotary\u001b[0;34m(x, cos, sin, seqlen_offsets, cu_seqlens, max_seqlen, interleaved, inplace, conjugate)\u001b[0m\n\u001b[1;32m    197\u001b[0m BLOCK_M \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interleaved \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;241m8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rotary_dim \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Need this, otherwise Triton tries to launch from cuda:0 and we get\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# ValueError: Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(x\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m    202\u001b[0m     rotary_kernel[grid](\n\u001b[1;32m    203\u001b[0m         output,  \u001b[38;5;66;03m# data ptrs\u001b[39;00m\n\u001b[1;32m    204\u001b[0m         x,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m         BLOCK_M,\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:365\u001b[0m, in \u001b[0;36mdevice.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_exchange_device(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument to exchangeDevice"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_ouput = model.generate(\n",
    "    **model_input.to(model.device),\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    min_new_tokens=0,\n",
    "    num_beams=1,\n",
    "    num_return_sequences=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f4a2a",
   "metadata": {},
   "source": [
    "### Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31808b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Mission</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">mention</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Ares 3\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">date</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"2032\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">departure</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Boca Chica\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">destination</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Mars\"</span>,\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Launcher</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">mention</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"Starship\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">space_company</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"SpaceX\"</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">crew</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">\"Max Rutherford\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Elena Soto\"</span>, <span style=\"color: #008000; text-decoration-color: #008000\">\"Jake Martinez\"</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mMission\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmention\u001b[0m=\u001b[32m\"Ares\u001b[0m\u001b[32m 3\"\u001b[0m,\n",
       "        \u001b[33mdate\u001b[0m=\u001b[32m\"2032\"\u001b[0m,\n",
       "        \u001b[33mdeparture\u001b[0m=\u001b[32m\"Boca\u001b[0m\u001b[32m Chica\"\u001b[0m,\n",
       "        \u001b[33mdestination\u001b[0m=\u001b[32m\"Mars\"\u001b[0m,\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mLauncher\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mmention\u001b[0m=\u001b[32m\"Starship\"\u001b[0m,\n",
       "        \u001b[33mspace_company\u001b[0m=\u001b[32m\"SpaceX\"\u001b[0m,\n",
       "        \u001b[33mcrew\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m\"Max Rutherford\"\u001b[0m, \u001b[32m\"Elena Soto\"\u001b[0m, \u001b[32m\"Jake Martinez\"\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[1m)\u001b[0m,\n",
       "\u001b[1m]\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for y, x in enumerate(model_ouput):\n",
    "    print(f\"Answer {y}\")\n",
    "    rich.print(tokenizer.decode(x,skip_special_tokens=True).split(\"result = \")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e2013f",
   "metadata": {},
   "source": [
    "## Parse the output\n",
    "\n",
    "The output is a Python list of instances, we can execute it  ðŸ¤¯\n",
    "\n",
    "We define the AnnotationList class to parse the output with a single line of code. The `AnnotationList.from_output` function filters any label that we did not define (hallucinations) to prevent getting an `undefined class` error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d66fb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Mission</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">mention</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Ares 3'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">date</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2032'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">departure</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Boca Chica'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">destination</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Mars'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Launcher</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">mention</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Starship'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">space_company</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'SpaceX'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">crew</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'Max Rutherford'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Elena Soto'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'Jake Martinez'</span><span style=\"font-weight: bold\">])</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mMission\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmention\u001b[0m=\u001b[32m'Ares 3'\u001b[0m, \u001b[33mdate\u001b[0m=\u001b[32m'2032'\u001b[0m, \u001b[33mdeparture\u001b[0m=\u001b[32m'Boca Chica'\u001b[0m, \u001b[33mdestination\u001b[0m=\u001b[32m'Mars'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mLauncher\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmention\u001b[0m=\u001b[32m'Starship'\u001b[0m, \u001b[33mspace_company\u001b[0m=\u001b[32m'SpaceX'\u001b[0m, \u001b[33mcrew\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'Max Rutherford'\u001b[0m, \u001b[32m'Elena Soto'\u001b[0m, \u001b[32m'Jake Martinez'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = AnnotationList.from_output(\n",
    "    tokenizer.decode(model_ouput[0],skip_special_tokens=True).split(\"result = \")[-1],\n",
    "    task_module=\"guidelines\"\n",
    "    )\n",
    "rich.print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27586bd8",
   "metadata": {},
   "source": [
    "Labels are an instance of the defined classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd039309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "guidelines.Mission"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2703325b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ares 3'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].mention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c52537",
   "metadata": {},
   "source": [
    "# Evaluate the result\n",
    "\n",
    "Finally, we will evaluate the outputs from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715535b7",
   "metadata": {},
   "source": [
    "First, we define an Scorer, for Named Entity Recognition, we will use the `SpanScorer` class.\n",
    "\n",
    "We need to define the `valid_types` for the scorer, which will be the labels that we have defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d44e1f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tasks.utils_scorer import TemplateScorer\n",
    "\n",
    "class MyScorer(TemplateScorer):\n",
    "    \"\"\"Compute the F1 score for Generic Task\"\"\"\n",
    "\n",
    "    valid_types: List[Type] = ENTITY_DEFINITIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9907289",
   "metadata": {},
   "source": [
    "### Instanciate the scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93d6c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = MyScorer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb11ce1",
   "metadata": {},
   "source": [
    "### Compute F1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da72e844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'templates'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'precision'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'recall'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'f1-score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'class_scores'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Launcher'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'tp'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_pos'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_pre'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'precision'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'recall'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'f1-score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'Mission'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'tp'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_pos'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'total_pre'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'precision'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'recall'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'f1-score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>\n",
       "            <span style=\"font-weight: bold\">}</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'slots'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'precision'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'recall'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'f1-score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'templates'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'precision'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "        \u001b[32m'recall'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "        \u001b[32m'f1-score'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "        \u001b[32m'class_scores'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'Launcher'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'tp'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'total_pos'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'total_pre'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'precision'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "                \u001b[32m'recall'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "                \u001b[32m'f1-score'\u001b[0m: \u001b[1;36m1.0\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'Mission'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'tp'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'total_pos'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'total_pre'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'precision'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "                \u001b[32m'recall'\u001b[0m: \u001b[1;36m1.0\u001b[0m,\n",
       "                \u001b[32m'f1-score'\u001b[0m: \u001b[1;36m1.0\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'slots'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'precision'\u001b[0m: \u001b[1;36m1.0\u001b[0m, \u001b[32m'recall'\u001b[0m: \u001b[1;36m1.0\u001b[0m, \u001b[32m'f1-score'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scorer_results = scorer(reference=[gold],predictions=[result])\n",
    "rich.print(scorer_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86d47e",
   "metadata": {},
   "source": [
    "GoLLIE has successfully labeled a sentence using a set of labels that were not part of the pretraining dataset ðŸŽ‰ðŸŽ‰ðŸŽ‰\n",
    "\n",
    "GoLLIE will perform well on labels with well-defined and clearly bounded guidelines. \n",
    "\n",
    "Please share your cool experiments with us; we'd love to see what everyone is doing with GoLLIE!\n",
    "- [@iker_garciaf](https://twitter.com/iker_garciaf)\n",
    "- [@osainz59](https://twitter.com/osainz59)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
